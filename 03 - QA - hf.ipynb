{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a187278-6cc8-4020-98e4-35f2ad0122f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "# model_name = \"GritLM/GritLM-7B\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"google/gemma-7b-it\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103816da-27f3-4540-b47d-9a53c92e0651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e20b5c-3ff7-49ce-9db5-8c4fe13f487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541d1d0949eb41c7b0ee1515e2bb552f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114f1e47dea146fe98857fbb097ed923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7095268354d86864a722275041955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c68d8fe9ab6431a9f0d058bec3dc86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b72fe04724642a28b672abe09bdd5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa690754a86441d91d8eb4f3535299b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c00b2061ca4d35b9d01538bc14907b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81792886d5ec4939a0d96fd4e2195910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7875c386b844a0eac814fea2aba259c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41787245ca8d4fde88912421c4817f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee41aa72b5144fd19ab86a884978b571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da6e7fa00554c949f00c5a1a9a9fed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b872428a28814089854dd981de14dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e28b0149-befe-4567-86a2-719d35c9ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size : 5708 MB\n",
      "Model is quantized : True\n",
      "Model quantization method : bitsandbytes\n",
      "Model 4-bit quantized : True\n",
      "Model on GPU : True\n"
     ]
    }
   ],
   "source": [
    "print(f'Model size : {model.get_memory_footprint() // 1024 ** 2} MB')\n",
    "\n",
    "try:\n",
    "    print(f'Model is quantized : {model.is_quantized}')\n",
    "    print(f'Model quantization method : {model.quantization_method}')\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    print(f'Model 8-bit quantized : {model.is_loaded_in_8bit}')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    print(f'Model 4-bit quantized : {model.is_loaded_in_4bit}')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f'Model on GPU : {next(model.parameters()).is_cuda}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95137c8-447f-4e95-966c-8a4a0a196ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>[INST] \n",
      "Paul is the speaker.\n",
      "List facts surrounded by the XML tag <fact>.\n",
      "List all discrete facts about Paul from the following text:\n",
      "My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna. [/INST]\n",
      "<fact>My daughter is 14 years old.</fact>\n",
      "<fact>She likes fencing, drawing, and playing games.</fact>\n",
      "<fact>Her name is Yuna.</fact>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "The text does not mention Paul, therefore I cannot extract the requested data.<eos>\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"[INST] \n",
    "Paul is the speaker.\n",
    "List facts surrounded by the XML tag <fact>.\n",
    "List all discrete facts about Paul from the following text:\n",
    "My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna. [/INST]\\n\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(message, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "_ = model.generate(\n",
    "    encoded,\n",
    "    streamer=streamer,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9a11ff-eac2-41fd-b25f-77d1b8f634d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>[INST] \n",
      "Paul is the speaker.\n",
      "List facts surrounded by the XML tag <fact>.\n",
      "List all discrete facts about Paul from the following coversation:\n",
      "assistant: How old is your daughter?\n",
      "user: She is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna. [/INST]\n",
      "**Answer:**\n",
      "<fact>She is 14 years old.</fact>\n",
      "<fact>Her name is Yuna.</fact>\n",
      "\n",
      "**Discrete facts about Paul:**\n",
      "\n",
      "The text does not provide any information about Paul, therefore I cannot provide the requested data.<eos>\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"[INST] \n",
    "Paul is the speaker.\n",
    "List facts surrounded by the XML tag <fact>.\n",
    "List all discrete facts about Paul from the following coversation:\n",
    "assistant: How old is your daughter?\n",
    "user: She is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna. [/INST]\\n\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(message, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "_ = model.generate(\n",
    "    encoded,\n",
    "    streamer=streamer,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=100,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb867fe-10e9-4757-964c-f9390f2aefb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "<input>My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna.</input>\n",
      "<instruction>List every fact provided by Paul in the input. List facts surrounded by the XML tag <fact>.</instruction>\n",
      "\n",
      "<fact>My daughter is 14 years old.</fact>\n",
      "<fact>She likes fencing, drawing, and playing games.</fact>\n",
      "<fact>Her name is Yuna.</fact>\n",
      "\n",
      "The above text has been rewritten with XML tags surrounding each fact to show the facts extracted from the text.<eos>\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"\n",
    "<input>My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna.</input>\n",
    "<instruction>List every fact provided by Paul in the input. List facts surrounded by the XML tag <fact>.</instruction>\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(message, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "_ = model.generate(\n",
    "    encoded,\n",
    "    streamer=streamer,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=100,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39324ead-fdf1-4b4b-a770-bcb3df93f530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>\n",
      "<input>My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna.</input>\n",
      "### Instruction:\n",
      "List every fact provided by Paul in the input. Be as detailed as possible.\n",
      "\n",
      "**Input:**\n",
      "My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna.\n",
      "\n",
      "**Output:**\n",
      "- My daughter is 14 years old.\n",
      "- She likes fencing, drawing, and playing games.\n",
      "- Her name is Yuna.\n",
      "```\n",
      "\n",
      "**Please provide me with the list of facts provided by Paul in the input.**\n",
      "\n",
      "**Answer:**\n",
      "- My daughter is 14 years old.\n",
      "- She likes fencing, drawing, and playing games.\n",
      "- Her name is Yuna.<eos>\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"\n",
    "<input>My daughter is 14 years old. She likes fencing, drawing, and playing games. Her name is Yuna.</input>\n",
    "### Instruction:\n",
    "List every fact provided by Paul in the input. Be as detailed as possible.\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(message, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "_ = model.generate(\n",
    "    encoded,\n",
    "    streamer=streamer,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=model.config.eos_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=100,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a9fff-9f74-4243-bf63-de7526132f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
